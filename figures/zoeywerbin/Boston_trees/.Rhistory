}
"
data <- list(x = x, y = y, n = length(y),
b0 = as.vector(c(0,0)),   ## regression b mean priors
Vb = solve(diag(10000,2)),   ## regression b precision priors
a0 = 0.1,                    ## error prior
Va = 0.1)                    ## error prior
## initial conditions
inits <- list()
for(i in 1:3){
inits[[i]] <- list(b = rnorm(2,0,5),
a = rlnorm(2,0.1,.1))}
j.model.he   <- jags.model (file = textConnection(he.model),
data = data,
inits = inits,
n.chains = 3)
jags.out.he   <- coda.samples (model = j.model.he,
variable.names = c("b","a"), # no likelihood till later so that our diagnostics don't get crowded
n.iter = 5000)
gelman.diag(jags.out.he) ## GBR stats are all less than 1.1, but could be better
acfplot(jags.out.he) ## Autocorrelation is high for parameter a[1], with a lag of at least 10
gelman.plot(jags.out.he) ## looks like we should remove about 5k as burn-in
burnin = 5000
jags.burn.he <- window(jags.out.he,start=burnin,thin=10)  ## remove burn-in, and thin by 10 samples
effectiveSize(jags.burn.he)
# re-running jags call to include likelihood, with same burn-in settings
jags.out.he2   <- coda.samples (model = j.model.he,
variable.names = c("b","a","like"),
n.iter = 20000, thin = 10)
jags.burn.he2 <- window(jags.out.he2,start=burnin)  ## remove burn-in
out.he2 <- as.matrix(jags.burn.he2)  ## Convert to matrix for later
DIC.he <- dic.samples(j.model.he, n.iter=5000)
DIC.he
# new storage matrices
ngibbs <- 5000
yobs  <- y[order(x)]
xpred <- x[order(x)]
npred <- length(xpred)
ypred <- matrix(NA,nrow=ngibbs,ncol=npred)
ycred <- matrix(NA,nrow=ngibbs,ncol=npred)
s <- matrix(NA,nrow=ngibbs,ncol=npred)
S <- matrix(NA,nrow=ngibbs,ncol=npred)
b1   <- out.he2[,grepl("b[1]",colnames(out.he2), fixed=T)]
b2   <- out.he2[,grepl("b[2]",colnames(out.he2), fixed=T)]
a1   <- out.he2[,grepl("a[1]",colnames(out.he2), fixed=T)]
a2   <- out.he2[,grepl("a[2]",colnames(out.he2), fixed=T)]
for(g in 1:ngibbs){
ycred[g,] <- b1[g] + b2[g] * xpred ## linear model on predictions
s[g,] <- a1[g] + a2[g]*xpred  ## linear model on standard deviation
#S[g,] <- 1/(s[g,]^2)          ## calculate precision from SD
ypred[g,] <- rnorm(npred,ycred[g,],s[g,])
}
## Residual variance
ybar <- apply(ycred,2,mean)
G <- sum((yobs-ybar)^2)/npred
## Predictive variance
P <- sum(apply(ypred,2,var))/npred
Dpl <- G + P
PL.he <- c(G,P,Dpl)
PL.he
like   <- out.he2[,grepl("^like",colnames(out.he2))]
fbar   <- colMeans(like)
Pw     <- sum(apply(log(like),2,var))
WAIC.he   <- -2*sum(log(fbar))+2*Pw
WAIC.he
xpred <- 1:10
npred <- length(xpred)
ypred <- matrix(NA,nrow=ngibbs,ncol=npred)
ycred <- matrix(NA,nrow=ngibbs,ncol=npred)
s <- matrix(NA,nrow=ngibbs,ncol=npred)
S <- matrix(NA,nrow=ngibbs,ncol=npred)
b1   <- out.he2[,grepl("b[1]",colnames(out.he2), fixed=T)]
b2   <- out.he2[,grepl("b[2]",colnames(out.he2), fixed=T)]
a1   <- out.he2[,grepl("a[1]",colnames(out.he2), fixed=T)]
a2   <- out.he2[,grepl("a[2]",colnames(out.he2), fixed=T)]
for(g in 1:ngibbs){
ycred[g,] <- b1[g] + b2[g] * xpred  ## linear model on predictions
s[g,]     <- a1[g] + a2[g] * xpred  ## linear model on standard deviation
#S[g,]     <- 1/(s[g,]^2)            ## calculate precision from SD
ypred[g,] <- rnorm(npred,ycred[g,],s[g,])
}
ci <- apply(ycred,2,quantile,c(0.025,0.975))  ## credible interval and median
pi <- apply(ypred,2,quantile,c(0.025,0.975))		## prediction interval
plot(x,y,cex=0.5,xlim=c(0,10),ylim=c(-10,30))
lines(xpred,ci[1,],col=3,lty=2)	## lower CI
lines(xpred,ci[2,],col=3,lty=2)	## upper CI
lines(xpred,pi[1,],col=4,lty=2)	## lower PI
lines(xpred,pi[2,],col=4,lty=2)	## upper PI
DIC.he
df <- data.frame(DIC = c(sum(DIC.ho$deviance), sum(DIC.he$deviance)),
WAIC = c(WAIC.ho, WAIC.he),
PL.G = c(PL.ho[[1]], PL.he[[1]]),
PL.P = c(PL.ho[[2]], PL.he[[2]]),
PL.D = c(PL.ho[[3]], PL.he[[3]]), row.names = c("homoskedastic","heteroskedastic")
)
df <- data.frame(DIC = c(sum(DIC.ho$deviance), sum(DIC.he$deviance)),
WAIC = c(WAIC.ho, WAIC.he),
PL.G = c(PL.ho[[1]], PL.he[[1]]),
PL.P = c(PL.ho[[2]], PL.he[[2]]),
PL.D = c(PL.ho[[3]], PL.he[[3]]), row.names = c("homoskedastic","heteroskedastic")
)
print(df)
df <- data.frame(DIC = c(sum(DIC.ho$deviance), sum(DIC.he$deviance)),
WAIC = c(WAIC.ho, WAIC.he),
PL.G = c(PL.ho[[1]], PL.he[[1]]),
PL.P = c(PL.ho[[2]], PL.he[[2]]),
PL.D = c(PL.ho[[3]], PL.he[[3]]), row.names = c("homoskedastic","heteroskedastic")
)
print(df)
15.92540/(29.82104)
12.06853/26.23555
knitr::opts_chunk$set(echo = TRUE)
ppt.mean
knitr::opts_chunk$set(echo = TRUE)
load("data/Ch11_UA.RData")
library(ecoforecastR)
plot(0,0,type = "n",xlim=c(0,NT),ylim = range(No),xlab="time",ylab="No")
for(s in 1:NS) {
points(No[s,],col=s,type='b')
}
plot(precip,type='b',xlab="time",ylab="precip (mm/yr)")
logisticRE <- "
model{
## priors
r_global ~ dnorm(0,0.1)     ## across-site mean growth rate
K_global ~ dlnorm(6,0.01)   ## across-site mean carrying capacity
beta ~ dnorm(0,0.000001)    ## slope of K response to precip
tau_site ~ dgamma(0.1,0.1)  ## site random effect precision
R ~ dgamma(0.01,0.00000001) ## Observation error precision
Q ~ dgamma(0.01,0.00000001) ## Process errror precision
## random effects and initial conditions, s = site
for(s in 1:NS){
alpha_site[s] ~ dnorm(0,tau_site)  ## random site effect on K
lN[s,1] ~ dnorm(6,0.001)           ## prior on IC, log scale
N[s,1] <- exp(lN[s,1])             ## IC, linear scale
}
## process model, t = time, s = site
for(t in 2:NT){
for(s in 1:NS){
## K is a linear model with a site random effect and fixed effect on log(precip)
K[s,t]  <- max(1,K_global+alpha_site[s]+beta*log(precip[t]/800))
## standard logistic growth process model, logged
mu[s,t] <- log(max(1,N[s,t-1] + r_global*N[s,t-1]*(1-N[s,t-1]/K[s,t])))
## process error
lN[s,t] ~ dnorm(mu[s,t],Q)
N[s,t] <- exp(lN[s,t])
}
}
## observation model
for(t in 1:NT){
for(s in 1:NS){
No[s,t] ~ dlnorm(lN[s,t],R)
}
}
}
"
## parameters
plot(out$params)
summary(out$params)
## states
ci <- apply(as.matrix(out$predict),2,quantile,c(0.025,0.5,0.975))
time = 1:NT
plot(0,0,type = "n",xlim=c(0,NT),ylim = range(ci),xlab="time",ylab="N")
for(s in 1:NS){
sel = seq(s,ncol(ci),by=NS)
ecoforecastR::ciEnvelope(time,ci[1,sel],ci[3,sel],col=col.alpha(s,0.6))
lines(time,ci[2,sel],col=s)
points(time,No[s,],col=s,pch=19)
points(time,No[s,])
}
### settings
s <- 6             ## Focal site for forward simulation
Nmc = 1000         ## set number of Monte Carlo draws
ylim = c(100,700)  ## set Y range on plot
N.cols <- c("black","red","green","blue","orange") ## set colors
trans <- 0.8       ## set transparancy
time = 1:(NT*2)    ## total time
time1 = 1:NT       ## calibration period
time2 = time1+NT   ## forecast period
plot.run <- function(){
sel = seq(s,ncol(ci),by=NS)
plot(time,time,type='n',ylim=ylim,ylab="N")
ecoforecastR::ciEnvelope(time1,ci[1,sel],ci[3,sel],col=col.alpha("lightBlue",0.6))
lines(time1,ci[2,sel],col="blue")
points(time1,No[s,])
}
ci <- apply(as.matrix(out$predict),2,quantile,c(0.025,0.5,0.975))
plot.run()
##` @param IC    Initial Conditions
##` @param r     Intrinsic growth rate
##` @param Kg    Across-site ('global') mean carrying capacity
##` @param alpha Site random effect
##` @param beta  Slope of precipitation effect on K
##` @param ppt   Precipitation forecast
##` @param Q     Process error (default = 0 for deterministic runs)
##` @param n     Size of Monte Carlo ensemble
forecastN <- function(IC,r,Kg,alpha,beta,ppt,Q=0,n=Nmc){
N <- matrix(NA,n,NT)  ## storage
Nprev <- IC           ## initialize
for(t in 1:NT){
K = pmax(1,Kg + alpha + beta*log(ppt[,t]/800))  ## calculate carrying capacity
mu = log(pmax(1,Nprev + r*Nprev*(1-Nprev/K)))   ## calculate mean
N[,t] <- rlnorm(n,mu,Q)                         ## predict next step
Nprev <- N[,t]                                  ## update IC
}
return(N)
}
## calculate mean of all inputs
ppt.mean <- matrix(apply(ppt_ensemble,2,mean),1,NT) ## driver
## parameters
params <- as.matrix(out$params)
param.mean <- apply(params,2,mean)
## initial conditions
IC <- as.matrix(out$predict)
N.det <- forecastN(IC=mean(IC[,"N[6,30]"]),
r=param.mean["r_global"],
Kg=param.mean["K_global"],
alpha=param.mean["alpha_site[6]"],
beta=param.mean["beta"],
ppt=ppt.mean,
Q=0,  ## process error off
n=1)
## Plot run
plot.run()
lines(time2,N.det,col="purple",lwd=3)
ppt.mean
ppt.mean
load("data/Ozone.RData")       ## data
load("data/Ozone.RData")       ## data
xt <- ts(ozone$Mean,start=1980,end=2008)  ## convert data to a time series object
plot(xt,type='b',ylab="ppm",xlab="Year",main="National Ozone Concentrations")
k <- c(0.1,0.2,0.4,0.2,0.1)      ## kernel
fx <- filter(xt,k)               ## weighted moving average
plot(xt,type='b',ylab="ppm",xlab="Year",main="National Ozone Concentrations")
lines(fx,col=3)
lx <- lowess(xt,f=1/3)
plot(xt,type='b',ylab="ppm",xlab="Year",main="National Ozone Concentrations")
lines(fx,col=3)
lines(lx,col=2)
rx = xt - lx$y        ## residuals around lowess curve
plot(rx)              ## check for homoskedasticity
abline(h=0,lty=2)
hist(rx,10)              ## check for a normal distribution with mean=zero
## Quantile-Quantile plot (by hand)
n = length(rx)
qseq = seq(0.5/n,length=n,by=1/n)
plot(qnorm(qseq,0,sd(rx)),sort(rx),main="Quantile-Quantile")
abline(0,1,lty=2)
dxt = diff(xt)
plot(dxt)
hist(dxt,10)
acf(xt)    ## ACF on the original time series
acf(rx)    ## ACF on the detrended data
acf(dxt)   ## ACF on the first difference series
pacf(xt)   ## Partial ACF of the time series
pacf(rx)   ## Partial ACF of the detrended data
pacf(dxt)  ## Partial ACF of the first differences
diff
arima(xt,c(0,0,1))
arima(xt,c(0,0,1))
arima(rx,c(0,1,0))
arima(xt,c(p,d,q))
arima(xt,c(0,0,1))
arima(xt,c(0,0,1))
arima(rx,c(0,1,0))
arima(rx,c(0,0,1))
arima(rx,c(1,1,1))
arima(rx,c(0,0,1))
ar(rx)
ar(xt)
arima(1,1,0)
arima(rx, 1,1,0)
arima(rx, c(1,1,0))
arima(xt, c(1,1,0))
arima(rx, c(2,1,0))
arima(rx, c(2,0,0))
arima(rx, c(2,0,1))
ar(rx)
ar(dxt)
arima(dxt, c(6,0,0))
arima(dxt, c(6,0,1))
arima(dxt, c(6,1,0))
arima(xt, c(2,1,1))
arima(dxt, c(6,0,0))
auto.arima(xt)
library(forecast)
auto.arima(xt)
auto.arima(rx)
arima(rx, c(0,0,0))
auto.arima(dxt)
arima(rx, c(0,0,0))
arima(rx, c(2,0,0))
data <- list(time = 1980:2008,y = ozone$Mean)
MLE.fit <- lm(y~time,data)
plot(y~time,data)
abline(MLE.fit,col=2)
data$H = as.matrix(dist(1:nrow(ozone),diag = TRUE,upper = TRUE))
data$H
library(rjags)
xt
data
mod <- "model{
library(rjags)
mod <- "model{
mod <- "model{
beta ~ dmnorm(b0,Vb)
sigma ~ dgamma(0.01,0.01)
rho ~ dunif(-1,1)
SIGMA <- inverse(1/sigma/(1-rho^2)*rho^H)
for(i in 1:n){
mu <- beta[1]+beta[2]*time
y ~ dmnorm(mu,SIGMA)
}
}"
j.model   <- jags.model (file = textConnection(mod),
data = data,
#                             inits = inits,
n.chains = 3)
mod <- "model{
beta ~ dmnorm(b0,Vb)
sigma ~ dgamma(0.01,0.01)
rho ~ dunif(-1,1)
SIGMA <- inverse(1/sigma/(1-rho^2)*rho^H)
mu <- beta[1]+beta[2]*time
y ~ dmnorm(mu,SIGMA)
}"
j.model   <- jags.model (file = textConnection(mod),
data = data,
#                             inits = inits,
n.chains = 3)
data$b0 <- as.vector(c(0,0))      ## regression beta means
data$Vb <- solve(diag(10000,2))   ## regression beta precisions
j.model   <- jags.model (file = textConnection(mod),
data = data,
#                             inits = inits,
n.chains = 3)
var.out   <- coda.samples (model = j.model,
variable.names = c("beta","SIGMA","rho"),
n.iter = 2000)
plot(var.out)
aic1 <- arima(rx,c(1,0,0))
aic2 <- arima(rx,c(0,1,0))
aic3 <- arima(rx,c(0,0,1))
aic4 <- arima(rx,c(1,1,1))
aic5 <- arima(xt,c(1,0,0))
aic6 <- arima(xt,c(0,2,0))
print(aic1)
print(aic2)
print(aic3)
print(aic4)
print(aic5)
print(aic6)
ar(xt)
ar(rx)
print(aic1)
print(aic2)
print(aic3)
print(aic4)
print(aic5)
print(aic6)
ar(xt)
ar(rx)
# By running ar(xt) and ar(rx), we see that the detrended data should have an order 2,
# and the un-detrended data should have order 1
arima(rx, c(2,0,0))
# By running ar(xt) and ar(rx), we see that the detrended data should have an order 2,
# and the un-detrended data should have order 1
aic7 <- arima(rx, c(2,0,0))
print(aic7)
GBR <- gelman.plot(j.model)
GBR <- gelman.plot(var.out)
var.out   <- coda.samples (model = j.model,
variable.names = c("beta","rho"),
n.iter = 5000)
arima(rx, c(2,0,0))
GBR <- gelman.plot(var.out)
acfplot(var.out)
GBR
gelman.plot(var.out)
gelman.plot(var.out)
acfplot(var.out)
jags.burn <- window(jags.out,start=3000)  ## remove burn-in
jags.burn <- window(var.out,start=3000)  ## remove burn-in
plot(jags.burn)                             ## check diagnostics post burn-in
load("data/Ozone.RData")       ## data
xt <- ts(ozone$Mean,start=1980,end=2008)  ## convert data to a time series object
plot(xt,type='b',ylab="ppm",xlab="Year",main="National Ozone Concentrations")
k <- c(0.1,0.2,0.4,0.2,0.1)      ## kernel
fx <- filter(xt,k)               ## weighted moving average
plot(xt,type='b',ylab="ppm",xlab="Year",main="National Ozone Concentrations")
lines(fx,col=3)
lx <- lowess(xt,f=1/3)
plot(xt,type='b',ylab="ppm",xlab="Year",main="National Ozone Concentrations")
lines(fx,col=3)
lines(lx,col=2)
rx = xt - lx$y        ## residuals around lowess curve
plot(rx)              ## check for homoskedasticity
abline(h=0,lty=2)
hist(rx,10)              ## check for a normal distribution with mean=zero
## Quantile-Quantile plot (by hand)
n = length(rx)
qseq = seq(0.5/n,length=n,by=1/n)
plot(qnorm(qseq,0,sd(rx)),sort(rx),main="Quantile-Quantile")
abline(0,1,lty=2)
dxt = diff(xt)
plot(dxt)
hist(dxt,10)
acf(xt)    ## ACF on the original time series
acf(rx)    ## ACF on the detrended data
acf(dxt)   ## ACF on the first difference series
pacf(xt)   ## Partial ACF of the time series
pacf(rx)   ## Partial ACF of the detrended data
pacf(dxt)  ## Partial ACF of the first differences
#arima(xt,c(p,d,q))
aic1 <- arima(rx,c(1,0,0))
aic2 <- arima(rx,c(0,1,0))
aic3 <- arima(rx,c(0,0,1))
aic4 <- arima(rx,c(1,1,1))
aic5 <- arima(xt,c(1,0,0))
aic6 <- arima(xt,c(0,2,0))
print(aic1)
print(aic2)
print(aic3)
print(aic4)
print(aic5)
print(aic6)
ar(xt)
ar(rx)
# By running ar(xt) and ar(rx), we see that the detrended data should have an order 2,
# and the un-detrended data should have order 1
aic7 <- arima(rx, c(2,0,0))
print(aic7)
aic1$aic
aic1$var.coef
aic1$code
aic1$model
aic1
aic1$call
df <- data.frame("Model" = c(aic1$call, aic2$call, aic3$call, aic4$call, aic5$call, aic6$call),
"AIC" = c(aic1$aic, aic2$aic, aic3$aic, aic4$aic, aic5$aic, aic6$aic))
aic1$aic
aic1$call
c(aic1$call, aic2$call, aic3$call, aic4$call, aic5$call, aic6$call)
c(aic1$aic, aic2$aic, aic3$aic, aic4$aic, aic5$aic, aic6$aic)
df <- cbind("Model" = c(aic1$call, aic2$call, aic3$call, aic4$call, aic5$call, aic6$call),
"AIC" = c(aic1$aic, aic2$aic, aic3$aic, aic4$aic, aic5$aic, aic6$aic))
df
df <- cbind("Model" = as.character(c(aic1$call, aic2$call, aic3$call, aic4$call, aic5$call, aic6$call)),
"AIC" = c(aic1$aic, aic2$aic, aic3$aic, aic4$aic, aic5$aic, aic6$aic))
df
df <- cbind("Model" = as.character(c(aic1$call, aic2$call, aic3$call, aic4$call, aic5$call, aic6$call)),
"AIC" = as.numeric(c(aic1$aic, aic2$aic, aic3$aic, aic4$aic, aic5$aic, aic6$aic)))
df
# By running ar(xt) and ar(rx), we see that the detrended data should have an order 2,
# and the un-detrended data should have order 1
aic7 <- arima(rx, c(2,0,0))
print(aic7)
#arima(xt,c(p,d,q))
aic1 <- arima(rx,c(1,0,0))
aic2 <- arima(rx,c(0,1,0))
aic3 <- arima(rx,c(0,0,1))
aic4 <- arima(rx,c(1,1,1))
aic5 <- arima(xt,c(1,0,0))
aic6 <- arima(xt,c(0,2,0))
df <- cbind("Model" = as.character(c(aic1$call, aic2$call, aic3$call, aic4$call, aic5$call, aic6$call)),
"AIC" = c(aic1$aic, aic2$aic, aic3$aic, aic4$aic, aic5$aic, aic6$aic))
print(df)
ar(xt)
ar(rx)
# By running ar(xt) and ar(rx), we see that the detrended data should have an order 2,
# and the un-detrended data should have order 1
aic7 <- arima(rx, c(2,0,0))
print(aic7)
data <- list(time = 1980:2008,y = ozone$Mean)
MLE.fit <- lm(y~time,data)
plot(y~time,data)
abline(MLE.fit,col=2)
data$H = as.matrix(dist(1:nrow(ozone),diag = TRUE,upper = TRUE))
data$H
library(rjags)
mod <- "model{
beta ~ dmnorm(b0,Vb)
sigma ~ dgamma(0.01,0.01)
rho ~ dunif(-1,1)
SIGMA <- inverse(1/sigma/(1-rho^2)*rho^H)
mu <- beta[1]+beta[2]*time
y ~ dmnorm(mu,SIGMA)
}"
data$b0 <- as.vector(c(0,0))      ## regression beta means
data$Vb <- solve(diag(10000,2))   ## regression beta precisions
j.model   <- jags.model (file = textConnection(mod),
data = data,
#                             inits = inits,
n.chains = 3)
var.out   <- coda.samples (model = j.model,
variable.names = c("beta","rho"),
n.iter = 5000)
gelman.plot(var.out)
acfplot(var.out)
jags.burn <- window(var.out,start=3000)  ## remove burn-in
plot(jags.burn)                             ## check diagnostics post burn-in
var.out   <- coda.samples (model = j.model,
variable.names = c("beta","rho","sigma"),
n.iter = 5000)
gelman.plot(var.out)
plot(jags.burn)                             ## check diagnostics post burn-in
jags.burn <- window(var.out,start=3000)  ## remove burn-in
acfplot(var.out)
setwd("~/Boston_trees")
shiny::runApp()
runApp()
shiny::runApp()
