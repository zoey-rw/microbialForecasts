---
title: 'Lab 08: Heteroskedasticity'
author: "EE509"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rjags)
```

## Objectives

In this lab we're going to:

* Explore putting process models on variances (heteroskedasticity)
* Explore Bayesian model selection

# Tasks

### Load & Plot Data

```{r}
load("data/Lab08_het.RData")
plot(x,y)
```

# Fit traditional linear model

Start from the basic linear model from lab 5. Fit the model to the data, perform you standard set of Bayesian metrics [trace plots, densities, GBR, pairs, effective sample size, etc.], and plot the resulting model (CI and PI) and data. When simulating your PI, make sure you've got the residual error in the right units (precision vs SD)

```{r}
univariate_regression <- "
model{

  b ~ dmnorm(b0,Vb)  	## multivariate Normal prior on vector of regression params
  S ~ dgamma(s1,s2)    ## prior precision

  for(i in 1:n){
	  mu[i] <- b[1] + b[2]*x[i]   	## process model
	  y[i]  ~ dnorm(mu[i],S)		        ## data model
    like[i] <- dnorm(y[i],mu[i],S)  # likelihood (added for tasks below)
  }
}
"

data <- list(x = x, y = y, n = length(y), 
             b0 = as.vector(c(0,0)),   ## regression b mean priors
             Vb = solve(diag(10000,2)),   ## regression b precision priors
             s1 = 0.1,                    ## error prior n/2
             s2 = 0.1)                    ## error prior SS/2
             
## initial conditions
inits <- list()
for(i in 1:3){
 inits[[i]] <- list(b = rnorm(2,0,5), S = runif(1,1/200,1/20))}

j.model   <- jags.model (file = textConnection(univariate_regression),
                             data = data,
                             inits = inits,
                             n.chains = 3)

jags.out   <- coda.samples (model = j.model,
                            variable.names = c("b","S"),
                                n.iter = 5000)

```

Check diagnostics:

```{r}
plot(jags.out) ## fuzzy caterpillars - good
gelman.diag(jags.out) ## values equal 1 - good
GBR <- gelman.plot(jags.out) ## determine convergence
burnin = 1500                                
jags.burn <- window(jags.out,start=burnin)  ## remove burn-in
```

Assess convergence after removing burn-in:
```{r}
plot(jags.burn)  # Good fuzzy caterpillers
gelman.diag(jags.burn) # GBR stats are all 1 - good
acfplot(jags.burn) # Autocorrelation is low
effectiveSize(jags.burn) # Effective size is over 10k, so that's high enough
```

View model output and summary:
```{r}
summary(jags.burn) 
out <- as.matrix(jags.burn)
pairs(out)	
cor(out)  
```
We see strong correlation between the b1 and b2 parameters, as is expected within a simple linear model.

## Plotting CI, PI, and data

Here we plot the credible and predictive intervals, using code adapted from Exercise 11 from the EF_activities repository and the Lesson 14 slides.
```{r}
b1 <- out[,2]
b2 <- out[,3]
S <- out[,1]

xpred <- 1:10
npred <- length(xpred)
ycred <- matrix(NA,nrow=nrow(out),ncol=10)
ypred <- matrix(NA,nrow=nrow(out),ncol=10)

for(g in 1:nrow(out)){
  ycred[g,] <- b1[g] + b2[g] * xpred
  ypred[g,] <- rnorm(npred,ycred[g,],1/sqrt(S[g]))
}

ci <- apply(ycred,2,quantile,c(0.025,0.975))  ## credible interval and median
pi <- apply(ypred,2,quantile,c(0.025,0.975))		## prediction interval
plot(x,y,cex=0.5,xlim=c(0,10),ylim=c(-10,30))
lines(xpred,ci[1,],col=3,lty=2)	## lower CI
lines(xpred,ci[2,],col=3,lty=2)	## upper CI
lines(xpred,pi[1,],col=4,lty=2)	## lower PI
lines(xpred,pi[2,],col=4,lty=2)	## upper PI
```

## Calculate model selection metrics

### DIC

```{r}
DIC.ho <- dic.samples(j.model, n.iter=5000)
DIC.ho
```

### WAIC

First, within the JAGS model, add the likelihood calculation within your for loop
```
 like[i] <- dnorm(y[i],mu[i],S)
```
Second, assuming that you've converted your JAGS output to a matrix to make the pairs plots and other diagnostics (e.g. `out <- as.matrix(jags.burn)`) we'll want to grab those likelihood columns to calculate WAIC. We'll do that using the `grepl` pattern matching function and the regular expression character `^` which tells R to find any column names that start with the following characters (in this case `like`). Once we do that we'll follow the same calculation as in the  

```{r}
# re-running jags call to include likelihood, with same burn-in settings
jags.out2   <- coda.samples (model = j.model,
                            variable.names = c("b","S","like"),
                                n.iter = 5000)
jags.burn2 <- window(jags.out2,start=burnin)  ## remove burn-in
out2 <- as.matrix(jags.burn2)

   like   <- out2[,grepl("^like",colnames(out2))] 
   fbar   <- colMeans(like)
   Pw     <- sum(apply(log(like),2,var))
   WAIC.ho   <- -2*sum(log(fbar))+2*Pw
   WAIC.ho
```
You'll also notice that out output now has a lot of `like` columns that complicate a lot of our other `coda` diagnostics. We can also use `grepl` to _exclude_ all the columns that have a pattern. For example:
```{r}
pairs(out2[,!grepl("^like",colnames(out2))])
```

### Predictive loss

The code for predictive loss is very similar to our code for generating confidence and predictive intervals, with the biggest different being that the calculations are done at the OBSERVED X's not a sequence of X's (though if you sort your X's you can often use that sequence to draw the CI & PI). 
```{r}
ngibbs = 3000
yobs  <- y[order(x)]
xpred <- x[order(x)]
npred <- length(xpred)
ypred <- matrix(NA,nrow=ngibbs,ncol=npred)
ycred <- matrix(NA,nrow=ngibbs,ncol=npred)
for(g in 1:ngibbs){
  ycred[g,] <- out[g,2] + out[g,3] * xpred
  ypred[g,] <- rnorm(npred,ycred[g,],sqrt(1/out[g,1]))
}

## Residual variance
ybar <- apply(ycred,2,mean)
G <- sum((yobs-ybar)^2)/npred

## Predictive variance
P <- sum(apply(ypred,2,var))/npred
Dpl <- G + P
PL.ho <- c(G,P,Dpl)
PL.ho
```
Note: for these metrics I've added `.ho` onto the end of the name for the homoskedastic model. For the heteroskedastic model you'll want to change this to something different (e.g. `.he`) so that you don't overwrite the results from your first models (you'll need both to make the table at the end)

# Fit heteroskedastic model 

To add heteroskedasticity, we'll start with the linear regression model and then modify it as follows:
* Within the JAGS `for` loop, add a process model for the calculation of the precision
```
  s[i] <- a[1] + a[2]*x[i]  ## linear model on standard deviation
  S[i] <- 1/s[i]^2          ## calculate precision from SD
```
* Replace prior on `S` with priors on `a[1]` and `a[2]`. To ensure that our variance is always positive, make sure to choose zero-bound prior distributions on `a`. Don't forget to add any new prior parameters to your `data` list.
* Update data model and WAIC likelihood calculation to use `S[i]` instead of a fixed `S`.
* Update your `coda.samples` to include `a` instead of `S`.
* As before, perform your standard MCMC metrics & diagnostics
* Calculate your three model selection metrics (DIC, WAIC, PL)
  ** For predictive loss, CI, and PI, don't forget to update your process model to include the process model on sigma and to make sure you're grabbing the right parameters! And don't forget the precision vs SD difference between R and JAGS.
* Plot your model and data with CI and PI
* As a final task, make a table that shows the different model selection metrics for both models. Briefly discuss how the metrics performed, what they told us, and where they are the same or different.

```{r}
he.model <- "
  model{

  b ~ dmnorm(b0,Vb)  	## multivariate Normal prior on vector of regression params
  a[1] ~ dlnorm(a0,Va)  	## log Normal prior on precision params
  a[2] ~ dlnorm(a0,Va)  	## log Normal prior on precision params

  for(i in 1:n){
	  mu[i] <- b[1] + b[2]*x[i]   	## process model
	  y[i]  ~ dnorm(mu[i],S[i])		  ## data model

    s[i] <- a[1] + a[2]*x[i]  ## linear model on standard deviation
    S[i] <- 1/s[i]^2          ## calculate precision from SD
    like[i] <- dnorm(y[i],mu[i],S[i]) # likelihood (added for WAIC)
  }
}
"

data <- list(x = x, y = y, n = length(y), 
             b0 = as.vector(c(0,0)),   ## regression b mean priors
             Vb = solve(diag(10000,2)),   ## regression b precision priors
             a0 = 0.1,                    ## error prior 
             Va = 0.1)                    ## error prior 

## initial conditions
inits <- list()
for(i in 1:3){
 inits[[i]] <- list(b = rnorm(2,0,5), 
                    a = rlnorm(2,0.1,.1))}

j.model.he   <- jags.model (file = textConnection(he.model),
                             data = data,
                             inits = inits,
                             n.chains = 3)

jags.out.he   <- coda.samples (model = j.model.he,
                              variable.names = c("b","a"), # no likelihood till later so that our diagnostics don't get crowded
                                n.iter = 5000)
```

Basic model diagnostics:
```{r}
gelman.diag(jags.out.he) ## GBR stats are all less than 1.1, but could be better
acfplot(jags.out.he) ## Autocorrelation is high for parameter a[1], with a lag of at least 10
gelman.plot(jags.out.he) ## looks like we should remove about 5k as burn-in
burnin = 5000                    
jags.burn.he <- window(jags.out.he,start=burnin,thin=10)  ## remove burn-in, and thin by 10 samples
effectiveSize(jags.burn.he) 
```

The effective size is very low (less than 1k), so let's take more samples, and include the "like" parameter for WAIC.
```{r}
# re-running jags call to include likelihood, with same burn-in settings
jags.out.he2   <- coda.samples (model = j.model.he,
                                variable.names = c("b","a","like"),
                                n.iter = 20000, thin = 10)
jags.burn.he2 <- window(jags.out.he2,start=burnin)  ## remove burn-in
out.he2 <- as.matrix(jags.burn.he2)  ## Convert to matrix for later
```


### DIC for heteroskedastic model

```{r}
DIC.he <- dic.samples(j.model.he, n.iter=5000)
DIC.he
```

### Predictive loss for heteroskedastic model

```{r}
# new storage matrices
ngibbs <- 5000
yobs  <- y[order(x)]
xpred <- x[order(x)]
npred <- length(xpred)
ypred <- matrix(NA,nrow=ngibbs,ncol=npred)
ycred <- matrix(NA,nrow=ngibbs,ncol=npred)
s <- matrix(NA,nrow=ngibbs,ncol=npred)

b1   <- out.he2[,grepl("b[1]",colnames(out.he2), fixed=T)] 
b2   <- out.he2[,grepl("b[2]",colnames(out.he2), fixed=T)] 
a1   <- out.he2[,grepl("a[1]",colnames(out.he2), fixed=T)] 
a2   <- out.he2[,grepl("a[2]",colnames(out.he2), fixed=T)] 

for(g in 1:ngibbs){
  ycred[g,] <- b1[g] + b2[g] * xpred ## linear model on predictions
  s[g,] <- a1[g] + a2[g]*xpred  ## linear model on standard deviation
  ypred[g,] <- rnorm(npred,ycred[g,],s[g,])
}
    
## Residual variance
ybar <- apply(ycred,2,mean)
G <- sum((yobs-ybar)^2)/npred

## Predictive variance
P <- sum(apply(ypred,2,var))/npred
Dpl <- G + P
PL.he <- c(G,P,Dpl)
PL.he
```

### WAIC for heteroskedastic model
```{r}
like   <- out.he2[,grepl("^like",colnames(out.he2))] 
fbar   <- colMeans(like)
Pw     <- sum(apply(log(like),2,var))
WAIC.he   <- -2*sum(log(fbar))+2*Pw
WAIC.he
```

Plot data and CI/PI using similar code from our predictive loss calculation:
```{r}
xpred <- 1:10
npred <- length(xpred)
ypred <- matrix(NA,nrow=ngibbs,ncol=npred)
ycred <- matrix(NA,nrow=ngibbs,ncol=npred)
s <- matrix(NA,nrow=ngibbs,ncol=npred)

b1   <- out.he2[,grepl("b[1]",colnames(out.he2), fixed=T)] 
b2   <- out.he2[,grepl("b[2]",colnames(out.he2), fixed=T)] 
a1   <- out.he2[,grepl("a[1]",colnames(out.he2), fixed=T)] 
a2   <- out.he2[,grepl("a[2]",colnames(out.he2), fixed=T)] 

for(g in 1:ngibbs){
  ycred[g,] <- b1[g] + b2[g] * xpred  ## linear model on predictions
  s[g,]     <- a1[g] + a2[g] * xpred  ## linear model on standard deviation
  ypred[g,] <- rnorm(npred,ycred[g,],s[g,])
}

ci <- apply(ycred,2,quantile,c(0.025,0.975))  ## credible interval and median
pi <- apply(ypred,2,quantile,c(0.025,0.975))		## prediction interval
plot(x,y,cex=0.5,xlim=c(0,10),ylim=c(-10,30))
lines(xpred,ci[1,],col=3,lty=2)	## lower CI
lines(xpred,ci[2,],col=3,lty=2)	## upper CI
lines(xpred,pi[1,],col=4,lty=2)	## lower PI
lines(xpred,pi[2,],col=4,lty=2)	## upper PI
```

Looks like our new predictive interval, from the heteroskedastic-variance model does a much better job at capturing our observed y values! 

The model selection results from the two models are also quite different:
```{r}

df <- data.frame(DIC = c(sum(DIC.ho$deviance), sum(DIC.he$deviance)), 
                 WAIC = c(WAIC.ho, WAIC.he), 
                 PL.G = c(PL.ho[[1]], PL.he[[1]]),
                 PL.P = c(PL.ho[[2]], PL.he[[2]]),
                 PL.D = c(PL.ho[[3]], PL.he[[3]]), row.names = c("homoskedastic","heteroskedastic")
                 )
print(df)

```
In this output, we see that the heteroskedastic-variance model performs better across each metric: it has lower DIC, lower WAIC, and lower total Predictive Loss (the last column, PL.D). DIC and WAIC take into account model complexity, so they indicate that the improvement brought by the additional heteroskedastic-model parameters is worth the parameter penalty.

Interestingly, the two models differ in the respective contributions of each component of Predictive Loss. PL.G (residual variance) is about the same for both models, because they have very similar mean estimates. However, in the homoskedastic model, the predictive variance (PL.P) is a higher proportion of PL.D (53%, compared to 46% for the heteroskedastic model). This reflects that treating variance as a function of x-values allows us to tailor our intervals to better suit the data - leading to a much tighter CI/PI at the lower x values, rather than the intervals being constant + wider to fit the spread at the high x values. 
