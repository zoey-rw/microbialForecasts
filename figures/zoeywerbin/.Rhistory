PL.D = c(PL.ho[[3]], PL.he[[3]]), row.names = c("homoskedastic","heteroskedastic")
)
DIC.he
knitr::opts_chunk$set(echo = TRUE)
library(rjags)
load("data/Lab08_het.RData")
plot(x,y)
univariate_regression <- "
model{
b ~ dmnorm(b0,Vb)  	## multivariate Normal prior on vector of regression params
S ~ dgamma(s1,s2)    ## prior precision
for(i in 1:n){
mu[i] <- b[1] + b[2]*x[i]   	## process model
y[i]  ~ dnorm(mu[i],S)		        ## data model
like[i] <- dnorm(y[i],mu[i],S)  # likelihood (added for tasks below)
}
}
"
data <- list(x = x, y = y, n = length(y),
b0 = as.vector(c(0,0)),   ## regression b mean priors
Vb = solve(diag(10000,2)),   ## regression b precision priors
s1 = 0.1,                    ## error prior n/2
s2 = 0.1)                    ## error prior SS/2
## initial conditions
inits <- list()
for(i in 1:3){
inits[[i]] <- list(b = rnorm(2,0,5), S = runif(1,1/200,1/20))}
j.model   <- jags.model (file = textConnection(univariate_regression),
data = data,
inits = inits,
n.chains = 3)
jags.out   <- coda.samples (model = j.model,
variable.names = c("b","S"),
n.iter = 5000)
plot(jags.out) ## fuzzy caterpillars - good
gelman.diag(jags.out) ## values equal 1 - good
GBR <- gelman.plot(jags.out) ## determine convergence
burnin = 1500
jags.burn <- window(jags.out,start=burnin)  ## remove burn-in
plot(jags.burn)  # Good fuzzy caterpillers
gelman.diag(jags.burn) # GBR stats are all 1 - good
acfplot(jags.burn) # Autocorrelation is low
effectiveSize(jags.burn) # Effective size is over 10k, so that's high enough
summary(jags.burn)
out <- as.matrix(jags.burn)
pairs(out)
cor(out)
b1 <- out[,2]
b2 <- out[,3]
S <- out[,1]
xpred <- 1:10
npred <- length(xpred)
ycred <- matrix(NA,nrow=nrow(out),ncol=10)
ypred <- matrix(NA,nrow=nrow(out),ncol=10)
for(g in 1:nrow(out)){
ycred[g,] <- b1[g] + b2[g] * xpred
ypred[g,] <- rnorm(npred,ycred[g,],1/sqrt(S[g]))
}
ci <- apply(ycred,2,quantile,c(0.025,0.975))  ## credible interval and median
pi <- apply(ypred,2,quantile,c(0.025,0.975))		## prediction interval
plot(x,y,cex=0.5,xlim=c(0,10),ylim=c(-10,30))
lines(xpred,ci[1,],col=3,lty=2)	## lower CI
lines(xpred,ci[2,],col=3,lty=2)	## upper CI
lines(xpred,pi[1,],col=4,lty=2)	## lower PI
lines(xpred,pi[2,],col=4,lty=2)	## upper PI
DIC.ho <- dic.samples(j.model, n.iter=5000)
DIC.ho
# re-running jags call to include likelihood, with same burn-in settings
jags.out2   <- coda.samples (model = j.model,
variable.names = c("b","S","like"),
n.iter = 5000)
jags.burn2 <- window(jags.out2,start=burnin)  ## remove burn-in
out2 <- as.matrix(jags.burn2)
like   <- out2[,grepl("^like",colnames(out2))]
fbar   <- colMeans(like)
Pw     <- sum(apply(log(like),2,var))
WAIC.ho   <- -2*sum(log(fbar))+2*Pw
WAIC.ho
pairs(out2[,!grepl("^like",colnames(out2))])
ngibbs = 3000
yobs  <- y[order(x)]
xpred <- x[order(x)]
npred <- length(xpred)
ypred <- matrix(NA,nrow=ngibbs,ncol=npred)
ycred <- matrix(NA,nrow=ngibbs,ncol=npred)
for(g in 1:ngibbs){
ycred[g,] <- out[g,2] + out[g,3] * xpred
ypred[g,] <- rnorm(npred,ycred[g,],sqrt(1/out[g,1]))
}
## Residual variance
ybar <- apply(ycred,2,mean)
G <- sum((yobs-ybar)^2)/npred
## Predictive variance
P <- sum(apply(ypred,2,var))/npred
Dpl <- G + P
PL.ho <- c(G,P,Dpl)
PL.ho
he.model <- "
model{
b ~ dmnorm(b0,Vb)  	## multivariate Normal prior on vector of regression params
a[1] ~ dlnorm(a0,Va)  	## log Normal prior on precision params
a[2] ~ dlnorm(a0,Va)  	## log Normal prior on precision params
for(i in 1:n){
mu[i] <- b[1] + b[2]*x[i]   	## process model
y[i]  ~ dnorm(mu[i],S[i])		  ## data model
s[i] <- a[1] + a[2]*x[i]  ## linear model on standard deviation
S[i] <- 1/s[i]^2          ## calculate precision from SD
like[i] <- dnorm(y[i],mu[i],S[i]) # likelihood (added for WAIC)
}
}
"
data <- list(x = x, y = y, n = length(y),
b0 = as.vector(c(0,0)),   ## regression b mean priors
Vb = solve(diag(10000,2)),   ## regression b precision priors
a0 = 0.1,                    ## error prior
Va = 0.1)                    ## error prior
## initial conditions
inits <- list()
for(i in 1:3){
inits[[i]] <- list(b = rnorm(2,0,5),
a = rlnorm(2,0.1,.1))}
j.model.he   <- jags.model (file = textConnection(he.model),
data = data,
inits = inits,
n.chains = 3)
jags.out.he   <- coda.samples (model = j.model.he,
variable.names = c("b","a"), # no likelihood till later so that our diagnostics don't get crowded
n.iter = 5000)
gelman.diag(jags.out.he) ## GBR stats are all less than 1.1, but could be better
acfplot(jags.out.he) ## Autocorrelation is high for parameter a[1], with a lag of at least 10
gelman.plot(jags.out.he) ## looks like we should remove about 5k as burn-in
burnin = 5000
jags.burn.he <- window(jags.out.he,start=burnin,thin=10)  ## remove burn-in, and thin by 10 samples
effectiveSize(jags.burn.he)
# re-running jags call to include likelihood, with same burn-in settings
jags.out.he2   <- coda.samples (model = j.model.he,
variable.names = c("b","a","like"),
n.iter = 20000, thin = 10)
jags.burn.he2 <- window(jags.out.he2,start=burnin)  ## remove burn-in
out.he2 <- as.matrix(jags.burn.he2)  ## Convert to matrix for later
DIC.he <- dic.samples(j.model.he, n.iter=5000)
DIC.he
# new storage matrices
ngibbs <- 5000
yobs  <- y[order(x)]
xpred <- x[order(x)]
npred <- length(xpred)
ypred <- matrix(NA,nrow=ngibbs,ncol=npred)
ycred <- matrix(NA,nrow=ngibbs,ncol=npred)
s <- matrix(NA,nrow=ngibbs,ncol=npred)
S <- matrix(NA,nrow=ngibbs,ncol=npred)
b1   <- out.he2[,grepl("b[1]",colnames(out.he2), fixed=T)]
b2   <- out.he2[,grepl("b[2]",colnames(out.he2), fixed=T)]
a1   <- out.he2[,grepl("a[1]",colnames(out.he2), fixed=T)]
a2   <- out.he2[,grepl("a[2]",colnames(out.he2), fixed=T)]
for(g in 1:ngibbs){
ycred[g,] <- b1[g] + b2[g] * xpred ## linear model on predictions
s[g,] <- a1[g] + a2[g]*xpred  ## linear model on standard deviation
#S[g,] <- 1/(s[g,]^2)          ## calculate precision from SD
ypred[g,] <- rnorm(npred,ycred[g,],s[g,])
}
## Residual variance
ybar <- apply(ycred,2,mean)
G <- sum((yobs-ybar)^2)/npred
## Predictive variance
P <- sum(apply(ypred,2,var))/npred
Dpl <- G + P
PL.he <- c(G,P,Dpl)
PL.he
like   <- out.he2[,grepl("^like",colnames(out.he2))]
fbar   <- colMeans(like)
Pw     <- sum(apply(log(like),2,var))
WAIC.he   <- -2*sum(log(fbar))+2*Pw
WAIC.he
xpred <- 1:10
npred <- length(xpred)
ypred <- matrix(NA,nrow=ngibbs,ncol=npred)
ycred <- matrix(NA,nrow=ngibbs,ncol=npred)
s <- matrix(NA,nrow=ngibbs,ncol=npred)
S <- matrix(NA,nrow=ngibbs,ncol=npred)
b1   <- out.he2[,grepl("b[1]",colnames(out.he2), fixed=T)]
b2   <- out.he2[,grepl("b[2]",colnames(out.he2), fixed=T)]
a1   <- out.he2[,grepl("a[1]",colnames(out.he2), fixed=T)]
a2   <- out.he2[,grepl("a[2]",colnames(out.he2), fixed=T)]
for(g in 1:ngibbs){
ycred[g,] <- b1[g] + b2[g] * xpred  ## linear model on predictions
s[g,]     <- a1[g] + a2[g] * xpred  ## linear model on standard deviation
#S[g,]     <- 1/(s[g,]^2)            ## calculate precision from SD
ypred[g,] <- rnorm(npred,ycred[g,],s[g,])
}
ci <- apply(ycred,2,quantile,c(0.025,0.975))  ## credible interval and median
pi <- apply(ypred,2,quantile,c(0.025,0.975))		## prediction interval
plot(x,y,cex=0.5,xlim=c(0,10),ylim=c(-10,30))
lines(xpred,ci[1,],col=3,lty=2)	## lower CI
lines(xpred,ci[2,],col=3,lty=2)	## upper CI
lines(xpred,pi[1,],col=4,lty=2)	## lower PI
lines(xpred,pi[2,],col=4,lty=2)	## upper PI
DIC.he
df <- data.frame(DIC = c(sum(DIC.ho$deviance), sum(DIC.he$deviance)),
WAIC = c(WAIC.ho, WAIC.he),
PL.G = c(PL.ho[[1]], PL.he[[1]]),
PL.P = c(PL.ho[[2]], PL.he[[2]]),
PL.D = c(PL.ho[[3]], PL.he[[3]]), row.names = c("homoskedastic","heteroskedastic")
)
df <- data.frame(DIC = c(sum(DIC.ho$deviance), sum(DIC.he$deviance)),
WAIC = c(WAIC.ho, WAIC.he),
PL.G = c(PL.ho[[1]], PL.he[[1]]),
PL.P = c(PL.ho[[2]], PL.he[[2]]),
PL.D = c(PL.ho[[3]], PL.he[[3]]), row.names = c("homoskedastic","heteroskedastic")
)
print(df)
df <- data.frame(DIC = c(sum(DIC.ho$deviance), sum(DIC.he$deviance)),
WAIC = c(WAIC.ho, WAIC.he),
PL.G = c(PL.ho[[1]], PL.he[[1]]),
PL.P = c(PL.ho[[2]], PL.he[[2]]),
PL.D = c(PL.ho[[3]], PL.he[[3]]), row.names = c("homoskedastic","heteroskedastic")
)
print(df)
15.92540/(29.82104)
12.06853/26.23555
knitr::opts_chunk$set(echo = TRUE)
ppt.mean
knitr::opts_chunk$set(echo = TRUE)
load("data/Ch11_UA.RData")
library(ecoforecastR)
plot(0,0,type = "n",xlim=c(0,NT),ylim = range(No),xlab="time",ylab="No")
for(s in 1:NS) {
points(No[s,],col=s,type='b')
}
plot(precip,type='b',xlab="time",ylab="precip (mm/yr)")
logisticRE <- "
model{
## priors
r_global ~ dnorm(0,0.1)     ## across-site mean growth rate
K_global ~ dlnorm(6,0.01)   ## across-site mean carrying capacity
beta ~ dnorm(0,0.000001)    ## slope of K response to precip
tau_site ~ dgamma(0.1,0.1)  ## site random effect precision
R ~ dgamma(0.01,0.00000001) ## Observation error precision
Q ~ dgamma(0.01,0.00000001) ## Process errror precision
## random effects and initial conditions, s = site
for(s in 1:NS){
alpha_site[s] ~ dnorm(0,tau_site)  ## random site effect on K
lN[s,1] ~ dnorm(6,0.001)           ## prior on IC, log scale
N[s,1] <- exp(lN[s,1])             ## IC, linear scale
}
## process model, t = time, s = site
for(t in 2:NT){
for(s in 1:NS){
## K is a linear model with a site random effect and fixed effect on log(precip)
K[s,t]  <- max(1,K_global+alpha_site[s]+beta*log(precip[t]/800))
## standard logistic growth process model, logged
mu[s,t] <- log(max(1,N[s,t-1] + r_global*N[s,t-1]*(1-N[s,t-1]/K[s,t])))
## process error
lN[s,t] ~ dnorm(mu[s,t],Q)
N[s,t] <- exp(lN[s,t])
}
}
## observation model
for(t in 1:NT){
for(s in 1:NS){
No[s,t] ~ dlnorm(lN[s,t],R)
}
}
}
"
## parameters
plot(out$params)
summary(out$params)
## states
ci <- apply(as.matrix(out$predict),2,quantile,c(0.025,0.5,0.975))
time = 1:NT
plot(0,0,type = "n",xlim=c(0,NT),ylim = range(ci),xlab="time",ylab="N")
for(s in 1:NS){
sel = seq(s,ncol(ci),by=NS)
ecoforecastR::ciEnvelope(time,ci[1,sel],ci[3,sel],col=col.alpha(s,0.6))
lines(time,ci[2,sel],col=s)
points(time,No[s,],col=s,pch=19)
points(time,No[s,])
}
### settings
s <- 6             ## Focal site for forward simulation
Nmc = 1000         ## set number of Monte Carlo draws
ylim = c(100,700)  ## set Y range on plot
N.cols <- c("black","red","green","blue","orange") ## set colors
trans <- 0.8       ## set transparancy
time = 1:(NT*2)    ## total time
time1 = 1:NT       ## calibration period
time2 = time1+NT   ## forecast period
plot.run <- function(){
sel = seq(s,ncol(ci),by=NS)
plot(time,time,type='n',ylim=ylim,ylab="N")
ecoforecastR::ciEnvelope(time1,ci[1,sel],ci[3,sel],col=col.alpha("lightBlue",0.6))
lines(time1,ci[2,sel],col="blue")
points(time1,No[s,])
}
ci <- apply(as.matrix(out$predict),2,quantile,c(0.025,0.5,0.975))
plot.run()
##` @param IC    Initial Conditions
##` @param r     Intrinsic growth rate
##` @param Kg    Across-site ('global') mean carrying capacity
##` @param alpha Site random effect
##` @param beta  Slope of precipitation effect on K
##` @param ppt   Precipitation forecast
##` @param Q     Process error (default = 0 for deterministic runs)
##` @param n     Size of Monte Carlo ensemble
forecastN <- function(IC,r,Kg,alpha,beta,ppt,Q=0,n=Nmc){
N <- matrix(NA,n,NT)  ## storage
Nprev <- IC           ## initialize
for(t in 1:NT){
K = pmax(1,Kg + alpha + beta*log(ppt[,t]/800))  ## calculate carrying capacity
mu = log(pmax(1,Nprev + r*Nprev*(1-Nprev/K)))   ## calculate mean
N[,t] <- rlnorm(n,mu,Q)                         ## predict next step
Nprev <- N[,t]                                  ## update IC
}
return(N)
}
## calculate mean of all inputs
ppt.mean <- matrix(apply(ppt_ensemble,2,mean),1,NT) ## driver
## parameters
params <- as.matrix(out$params)
param.mean <- apply(params,2,mean)
## initial conditions
IC <- as.matrix(out$predict)
N.det <- forecastN(IC=mean(IC[,"N[6,30]"]),
r=param.mean["r_global"],
Kg=param.mean["K_global"],
alpha=param.mean["alpha_site[6]"],
beta=param.mean["beta"],
ppt=ppt.mean,
Q=0,  ## process error off
n=1)
## Plot run
plot.run()
lines(time2,N.det,col="purple",lwd=3)
ppt.mean
ppt.mean
load("data/Ozone.RData")       ## data
load("data/Ozone.RData")       ## data
xt <- ts(ozone$Mean,start=1980,end=2008)  ## convert data to a time series object
plot(xt,type='b',ylab="ppm",xlab="Year",main="National Ozone Concentrations")
k <- c(0.1,0.2,0.4,0.2,0.1)      ## kernel
fx <- filter(xt,k)               ## weighted moving average
plot(xt,type='b',ylab="ppm",xlab="Year",main="National Ozone Concentrations")
lines(fx,col=3)
lx <- lowess(xt,f=1/3)
plot(xt,type='b',ylab="ppm",xlab="Year",main="National Ozone Concentrations")
lines(fx,col=3)
lines(lx,col=2)
rx = xt - lx$y        ## residuals around lowess curve
plot(rx)              ## check for homoskedasticity
abline(h=0,lty=2)
hist(rx,10)              ## check for a normal distribution with mean=zero
## Quantile-Quantile plot (by hand)
n = length(rx)
qseq = seq(0.5/n,length=n,by=1/n)
plot(qnorm(qseq,0,sd(rx)),sort(rx),main="Quantile-Quantile")
abline(0,1,lty=2)
dxt = diff(xt)
plot(dxt)
hist(dxt,10)
acf(xt)    ## ACF on the original time series
acf(rx)    ## ACF on the detrended data
acf(dxt)   ## ACF on the first difference series
pacf(xt)   ## Partial ACF of the time series
pacf(rx)   ## Partial ACF of the detrended data
pacf(dxt)  ## Partial ACF of the first differences
diff
arima(xt,c(0,0,1))
arima(xt,c(0,0,1))
arima(rx,c(0,1,0))
arima(xt,c(p,d,q))
arima(xt,c(0,0,1))
arima(xt,c(0,0,1))
arima(rx,c(0,1,0))
arima(rx,c(0,0,1))
arima(rx,c(1,1,1))
arima(rx,c(0,0,1))
ar(rx)
ar(xt)
arima(1,1,0)
arima(rx, 1,1,0)
arima(rx, c(1,1,0))
arima(xt, c(1,1,0))
arima(rx, c(2,1,0))
arima(rx, c(2,0,0))
arima(rx, c(2,0,1))
ar(rx)
ar(dxt)
arima(dxt, c(6,0,0))
arima(dxt, c(6,0,1))
arima(dxt, c(6,1,0))
arima(xt, c(2,1,1))
arima(dxt, c(6,0,0))
auto.arima(xt)
library(forecast)
auto.arima(xt)
auto.arima(rx)
arima(rx, c(0,0,0))
auto.arima(dxt)
arima(rx, c(0,0,0))
arima(rx, c(2,0,0))
data <- list(time = 1980:2008,y = ozone$Mean)
MLE.fit <- lm(y~time,data)
plot(y~time,data)
abline(MLE.fit,col=2)
data$H = as.matrix(dist(1:nrow(ozone),diag = TRUE,upper = TRUE))
data$H
library(rjags)
xt
data
mod <- "model{
library(rjags)
mod <- "model{
mod <- "model{
beta ~ dmnorm(b0,Vb)
sigma ~ dgamma(0.01,0.01)
rho ~ dunif(-1,1)
SIGMA <- inverse(1/sigma/(1-rho^2)*rho^H)
for(i in 1:n){
mu <- beta[1]+beta[2]*time
y ~ dmnorm(mu,SIGMA)
}
}"
j.model   <- jags.model (file = textConnection(mod),
data = data,
#                             inits = inits,
n.chains = 3)
mod <- "model{
beta ~ dmnorm(b0,Vb)
sigma ~ dgamma(0.01,0.01)
rho ~ dunif(-1,1)
SIGMA <- inverse(1/sigma/(1-rho^2)*rho^H)
mu <- beta[1]+beta[2]*time
y ~ dmnorm(mu,SIGMA)
}"
j.model   <- jags.model (file = textConnection(mod),
data = data,
#                             inits = inits,
n.chains = 3)
data$b0 <- as.vector(c(0,0))      ## regression beta means
data$Vb <- solve(diag(10000,2))   ## regression beta precisions
j.model   <- jags.model (file = textConnection(mod),
data = data,
#                             inits = inits,
n.chains = 3)
var.out   <- coda.samples (model = j.model,
variable.names = c("beta","SIGMA","rho"),
n.iter = 2000)
plot(var.out)
aic1 <- arima(rx,c(1,0,0))
aic2 <- arima(rx,c(0,1,0))
aic3 <- arima(rx,c(0,0,1))
aic4 <- arima(rx,c(1,1,1))
aic5 <- arima(xt,c(1,0,0))
aic6 <- arima(xt,c(0,2,0))
print(aic1)
print(aic2)
print(aic3)
print(aic4)
print(aic5)
print(aic6)
ar(xt)
ar(rx)
print(aic1)
print(aic2)
print(aic3)
print(aic4)
print(aic5)
print(aic6)
ar(xt)
ar(rx)
# By running ar(xt) and ar(rx), we see that the detrended data should have an order 2,
# and the un-detrended data should have order 1
arima(rx, c(2,0,0))
# By running ar(xt) and ar(rx), we see that the detrended data should have an order 2,
# and the un-detrended data should have order 1
aic7 <- arima(rx, c(2,0,0))
print(aic7)
GBR <- gelman.plot(j.model)
GBR <- gelman.plot(var.out)
var.out   <- coda.samples (model = j.model,
variable.names = c("beta","rho"),
n.iter = 5000)
arima(rx, c(2,0,0))
GBR <- gelman.plot(var.out)
acfplot(var.out)
GBR
gelman.plot(var.out)
gelman.plot(var.out)
acfplot(var.out)
jags.burn <- window(jags.out,start=3000)  ## remove burn-in
jags.burn <- window(var.out,start=3000)  ## remove burn-in
plot(jags.burn)                             ## check diagnostics post burn-in
remotes::install_github("serrat839/mRkov")
shiny::runGithub("mRkov_shiny", username = "serrat839")
shiny:::runGithub("mRkov_shiny", username = "serrat839")
library(shiny)
shiny::runGithub("mRkov_shiny", username = "serrat839")
shiny:::runGithub("mRkov_shiny", username = "serrat839")
shiny::runGitHub("mRkov_shiny", username = "serrat839")
shiny::runGitHub("mRkov_shiny", username = "serrat839")
shiny::runGitHub("mRkov_shiny", username="serrat839")
library(mRkov)
tweets <- tweet_gettr("@realDonaldTrump")
tweets <- tweet_gettr("@johnluvsolivoil")
library(devtools)
install_github("serrat839/mRkov")
library(mRkov)
# Select a twitter handle and save to a variable. API keys are pre-loaded for a dummy twitter account.
tweets <- tweet_gettr("@realDonaldTrump")
tweets <- tweet_gettr("@johnluvsolivoil")
shiny::runApp('Boston_trees')
