---
title: "Results_Exercise5_ZW_02-25-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rjags)
library(coda)
```

## Case Study: Forest Stand Characteristics

For the next few examples we'll be using a dataset on the diameters of loblolly pine trees at the Duke FACE experiment. In this example we'll just be looking at the diameter data in order to characterize the stand itself. Let's begin by expanding the model we specified above to account for a large dataset (rather than a single observation), in order to estimate the mean stem diameter at the site. As a first step let's still assume that the variance is known. Our data set has 297 values, so when specifying the model in JAGS we'll need to loop over each value to calculate the likelihood of each data point and use the vector index notation, [i] , to specify which value we're computing.

```{r}
NormalMeanN <- "
model {
mu ~ dnorm(mu0,T) # prior on the mean 
for(i in 1:N){
X[i] ~ dnorm(mu,S) # data model
}
}
"
```

The data for fiting this model are

```{r}
data = list(
  N = 297, 
  mu0=20, 
  T=0.01, 
  S = 1/27, 
  X = c(20.9, 13.6, 15.7, 6.3, 2.7, 25.6, 4, 20.9, 7.8, 27.1, 25.2, 19, 17.8, 22.8, 12.5, 21.1, 22, 22.4, 5.1, 16, 20.7, 15.7, 5.5, 18.9, 22.9, 15.5, 18.6, 19.3, 14.2, 12.3, 11.8, 26.8, 17, 5.7, 12, 19.8, 19, 23.6, 19.9, 8.4, 22, 18.1, 21.6, 17, 12.4, 2.9, 22.6, 20.8, 18.2, 14.2, 17.3, 14.5, 8.6, 9.1, 2.6, 19.8, 20, 22.2, 10.2, 12.9, 20.9, 21.1, 7.3, 5.8, 23.1, 17, 21.5, 10.1, 18.4, 22.6, 21.2, 21.5, 22.4, 17.3, 16, 25, 22.4, 23.9, 23, 21.9, 19, 28.6, 16, 22.5, 23.2, 8.7, 23.4, 15.3, 25.6, 19.2, 17.4, 23.8, 20.4, 19, 3.6, 23.4, 19.6, 17.5, 16.5, 22, 19.7, 7.35, 18, 17.8, 9.6, 15, 12, 17.7, 21.4, 17, 22.1, 18.9, 15.05, 12.9, 19.3, 15.3, 13.6, 15.4, 10.6, 11.3, 11.8, 22.2, 22.2, 13.1, 7.4, 4.5, 11.7, 19.5, 19.9, 11.6, 13.9, 15.5, 11, 18.6, 17.6, 12.7, 20.9, 18.8, 22.4, 21.2, 18.2, 15.3, 13.6, 7.3, 17.4, 17.4, 10.5, 22.9, 23.2, 13.8, 14.8, 22.2, 20.9, 13, 18.9, 19, 15.2, 16.8, 18, 24.6, 15.4, 17.2, 23.2, 22.8, 25.5, 7.8, 6, 6.4, 19, 13.5, 23.7, 18, 22.2, 22.4, 9.3, 13.7, 18.9, 20.5, 23.3, 20.8, 18.4, 4.5, 12.2, 16.9, 13.5, 17.8, 16.9, 20.4, 19.5, 22.2, 24.5, 21.2, 16.5, 18, 16.4, 3.9, 17.9, 22, 12.9, 21, 18, 9.2, 15.9, 8.1, 8.3, 10.7, 12, 19.9, 13.6, 17.3, 11.5, 12.4, 15.1, 22, 19.3, 17.5, 14.5, 14.7, 17.5, 19.6, 12.9, 20.3, 17.9, 20.2, 18.3, 9.5, 19, 21, 13.1, 20.4, 16.3, 18.3, 11.8, 23.3, 15.2, 20, 17.9, 12, 19.6, 18.5, 16.2, 10.9, 17.8, 13.8, 10, 17.9, 15.6, 20.3, 14.9, 18.6, 12.5, 18.2, 16, 18.7, 18, 15.3, 19, 17.9, 15.8, 17.7, 14.4, 19.6, 18.3, 18.7, 17.8, 18, 10.1, 18.8, 16.4, 21.2, 16.6, 16.7, 17.8, 16.5, 19.3, 16.3, 14.2, 13, 9.4, 19.7, 13.4, 2.6, 17.6, 16.7, 17.6, 5.8, 17.6, 20.1, 18.2, 16.7, 14, 13.9, 5.1, 16.6, 3.9, 17.5, 18)
)
```

# Activity Task 1
Run the unknown mean/fixed variance model to estimate mean tree diameter. Include the following in your results:
  
  * Table of summary statistics for the posterior estimate of the mean
* Graph of parameter density
* Plot of MCMC “history”
* Length of your MCMC (i.e. the total number of “updates”), the number of chains, the burnin values you used, the effective sample size, and any graphs/statistics/rationale you used to justify those settings

```{r}
inits <- list()
inits[[1]] <- list(mu = 15)
inits[[2]] <- list(mu = 20)
inits[[3]] <- list(mu = 25)
j.model   <- jags.model (file = textConnection(NormalMeanN),
                             data = data,
                             inits = inits,
                             n.chains = 3)
```
###Graph of parameter density and plot of MCMC “history” (trace plot)
```{r}
jags.out   <- coda.samples (model = j.model,
                            variable.names = c("mu"),
                                n.iter = 1000)
plot(jags.out)
```
#### Check convergence. 
```{r}
gelman.diag(jags.out)
```
Both values are very close to 1, so model has converged.

### Determine amount of burn-in to discard. 
```{r}
GBR <- gelman.plot(jags.out)
```
Looks like 400 is enough.

### Discard burn-in and check that we removed enough.
```{r}
burnin = 400                                ## determine convergence
jags.burn <- window(jags.out,start=burnin)  ## remove burn-in
plot(jags.burn)                             ## check diagnostics post burn-in
```

### Check autocorrelation and effective sample size.
```{r}
acfplot(jags.burn)
effectiveSize(jags.burn)
```
Effective sample size is ~1722, which is lower than our number of samples. Let's draw more:
```{r}
jags.out2 <- coda.samples(j.model,
                          variable.names = c("mu"),
                          10000)
```


###Table of summary statistics for the posterior estimate of the mean:
```{r}
summary(jags.out2)
```









# Activity Task 2
Modify the model to account for the uncertainty in the variance. This only requires adding one line — a prior on S outside the loop.

Run the unknown mean/unknown variance model to simultaneously estimate both the mean tree diameter and the standard deviation. Include the following in your results:
  
  * Explaination of your choice of prior on the precision
* Table of summary statistics for the posterior mean and standard deviation
* Graph of parameter densities
* Plot of MCMC “history”
* Length of your MCMC (i.e. the total number of “updates”), the number of chains, burnin, effective sample size, and any graphs/statistics/rationale you used to justify those settings
* Also describe any changes in the distribution of the mean (shape, location, tails) compared to Task 1.

```{r}
data = list(
  N = 297, 
  mu0=20, 
  T=0.01, 
  S0 = 1,
  p = .1, 
  X = c(20.9, 13.6, 15.7, 6.3, 2.7, 25.6, 4, 20.9, 7.8, 27.1, 25.2, 19, 17.8, 22.8, 12.5, 21.1, 22, 22.4, 5.1, 16, 20.7, 15.7, 5.5, 18.9, 22.9, 15.5, 18.6, 19.3, 14.2, 12.3, 11.8, 26.8, 17, 5.7, 12, 19.8, 19, 23.6, 19.9, 8.4, 22, 18.1, 21.6, 17, 12.4, 2.9, 22.6, 20.8, 18.2, 14.2, 17.3, 14.5, 8.6, 9.1, 2.6, 19.8, 20, 22.2, 10.2, 12.9, 20.9, 21.1, 7.3, 5.8, 23.1, 17, 21.5, 10.1, 18.4, 22.6, 21.2, 21.5, 22.4, 17.3, 16, 25, 22.4, 23.9, 23, 21.9, 19, 28.6, 16, 22.5, 23.2, 8.7, 23.4, 15.3, 25.6, 19.2, 17.4, 23.8, 20.4, 19, 3.6, 23.4, 19.6, 17.5, 16.5, 22, 19.7, 7.35, 18, 17.8, 9.6, 15, 12, 17.7, 21.4, 17, 22.1, 18.9, 15.05, 12.9, 19.3, 15.3, 13.6, 15.4, 10.6, 11.3, 11.8, 22.2, 22.2, 13.1, 7.4, 4.5, 11.7, 19.5, 19.9, 11.6, 13.9, 15.5, 11, 18.6, 17.6, 12.7, 20.9, 18.8, 22.4, 21.2, 18.2, 15.3, 13.6, 7.3, 17.4, 17.4, 10.5, 22.9, 23.2, 13.8, 14.8, 22.2, 20.9, 13, 18.9, 19, 15.2, 16.8, 18, 24.6, 15.4, 17.2, 23.2, 22.8, 25.5, 7.8, 6, 6.4, 19, 13.5, 23.7, 18, 22.2, 22.4, 9.3, 13.7, 18.9, 20.5, 23.3, 20.8, 18.4, 4.5, 12.2, 16.9, 13.5, 17.8, 16.9, 20.4, 19.5, 22.2, 24.5, 21.2, 16.5, 18, 16.4, 3.9, 17.9, 22, 12.9, 21, 18, 9.2, 15.9, 8.1, 8.3, 10.7, 12, 19.9, 13.6, 17.3, 11.5, 12.4, 15.1, 22, 19.3, 17.5, 14.5, 14.7, 17.5, 19.6, 12.9, 20.3, 17.9, 20.2, 18.3, 9.5, 19, 21, 13.1, 20.4, 16.3, 18.3, 11.8, 23.3, 15.2, 20, 17.9, 12, 19.6, 18.5, 16.2, 10.9, 17.8, 13.8, 10, 17.9, 15.6, 20.3, 14.9, 18.6, 12.5, 18.2, 16, 18.7, 18, 15.3, 19, 17.9, 15.8, 17.7, 14.4, 19.6, 18.3, 18.7, 17.8, 18, 10.1, 18.8, 16.4, 21.2, 16.6, 16.7, 17.8, 16.5, 19.3, 16.3, 14.2, 13, 9.4, 19.7, 13.4, 2.6, 17.6, 16.7, 17.6, 5.8, 17.6, 20.1, 18.2, 16.7, 14, 13.9, 5.1, 16.6, 3.9, 17.5, 18)
)
```
Here, we add a prior on S. The prior is given a Normal distribution based on S0 and p, values passed in in the data list, which were 1 and 0.1 respectively. These values were chosen because we don't want a negative precision, and we don't know enough about the system to choose a more informative prior. This is a weakly informative prior on precision, because it should be dominated by the precision of the actual data. 
```{r}
NormalMeanN <- "
model {
mu ~ dnorm(mu0,T) # prior on the mean 
S ~ dnorm(S0, p)
for(i in 1:N){
X[i] ~ dnorm(mu,S) # data model
}
}
"
```


```{r}

j.model   <- jags.model (file = textConnection(NormalMeanN),
                             data = data)
```
```{r}
inits <- list()
inits[[1]] <- list(mu = 15, S = .5)
inits[[2]] <- list(mu = 20, S = 1)
inits[[3]] <- list(mu = 25, S = 1.5)
j.model   <- jags.model (file = textConnection(NormalMeanN),
                             data = data,
                             inits = inits,
                             n.chains = 3)
```
###Graph of parameter density and plot of MCMC “history” (trace plot)
```{r}
jags.out   <- coda.samples (model = j.model,
                            variable.names = c("mu", "S"),
                                n.iter = 1000)
plot(jags.out)
```
#### Check convergence. 
```{r}
gelman.diag(jags.out)
```
Both values are very close to 1, so model has converged.

### Determine amount of burn-in to discard. 
```{r}
GBR <- gelman.plot(jags.out)
```
Looks like 400 is enough for this model, too.

### Discard burn-in and check that we removed enough.
```{r}
burnin = 400                                ## determine convergence
jags.burn <- window(jags.out,start=burnin)  ## remove burn-in
plot(jags.burn)                             ## check diagnostics post burn-in
```

### Check autocorrelation and effective sample size.
```{r}
acfplot(jags.burn)
effectiveSize(jags.burn)
```
Effective sample size is almost 3000 for mu, which is lower than our number of samples, so let's draw more:
```{r}
jags.out2 <- coda.samples(j.model,
                          variable.names = c("mu", "S"),
                          10000)
```


###Table of summary statistics for the posterior estimate of the mean:
```{r}
summary(jags.out2)
```

### How did the distribution change when an unknown variance was added?

With an unknown variance, the effective sample size was higher, but the mean still converged on a value of 16.57. The density plots of mu also looked similar, with most values between 15.5 and 17.5, but with slightly less spread (tails didn't extend as far above or below the mean).