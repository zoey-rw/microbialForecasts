---
title: "Untitled"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
## since libraries will be pulled, make sure repository is set
repos = "http://cran.us.r-project.org"
get.pkg <- function(pkg){
  loaded <- do.call("require",list(package=pkg))
  if(!loaded){
    print(paste("trying to install",pkg))
    install.packages(pkg,dependencies=TRUE,repos=repos)
    loaded <- do.call("require",list(package=pkg))
    if(loaded){
      print(paste(pkg,"installed and loaded"))
    } 
    else {
      stop(paste("could not install",pkg))
    }    
  }
}
get.pkg("RCurl")
get.pkg("XML")
get.pkg("ncdf4")
get.pkg("devtools")
get.pkg("MODISTools")
```


##Question 1:
  
  Using the US Forest Service's Forest Inventory and Analysis (FIA) data set, plot the rank vs log(abundance) curve for tree seedling counts from Rhode Island. Data is available at https://apps.fs.usda.gov/fia/datamart/CSV/RI_SEEDLING.csv and the relevant columns are TREECOUNT (raw seedling counts) and SPCD (species codes). 
Hints: tapply, sum, na.rm=TRUE, sort, decreasing=TRUE, log='y'

```{r}
library(plyr)

fia.raw = read.csv("https://apps.fs.usda.gov/fia/datamart/CSV/RI_SEEDLING.csv")
fia.df <- fia.raw[,c("TREECOUNT_CALC", "SPCD")]
treesums <- ddply(fia.df, .(SPCD), numcolwise(sum))
plot(sort(treesums$TREECOUNT_CALC, decreasing=T), 
     log='y',type='l', ylab="log(Species abundance)", xlab="Rank", 
     main="Rank-abundance curve for tree seedling counts in RI")
```

## Question 2
Create a sorted table of how many FLUXNET eddy-covariance towers are in each country according to the website at http://fluxnet.fluxdata.org/sites/site-list-and-pages/. Hint: use substring to extract the country code from the overall FLUXNET ID code.

```{r}
library(rvest)
flux_html <- getURL("http://fluxnet.fluxdata.org/sites/site-list-and-pages/")  ## grab raw html
fia_table = readHTMLTable(flux_html)

# tried other methods in https://stackoverflow.com/questions/1395528/scraping-html-tables-into-r-data-frames-using-the-xml-package but nothing works to scrape the table.

```


## Question 3 

Within the object myCode, find all the lines that begin with the comment character, #.

```{r}
myCode = readLines("Exercise_03_BigData.Rmd")  ## read unstructured text
x = grep("^#",myCode)    ## returns the line numbers that start with "#"
myCode[x]
```

## Question 4 

Similar to how we can point read.csv to the URL of a text file, you can open and manipulate netCDF files on remote servers if those servers support THREDDS/OpenDAP. Furthermore, these utilities let you grab just the part of the file that you need rather than the file in it's entirety. Using this approach, download and plot the air temperature data for Boston for 2004 that's located on the ORNL DAAC server `http://thredds.daac.ornl.gov/thredds/dodsC/ornldaac/1220/mstmip_driver_global_hd_climate_tair_2004_v1.nc4`.  The underlying file is quite large so make sure to grab just the subset you need. To do so you'll need to first grab the lat, lon, and time variables to find _which_ grid cell to grab for lat and lon and how many values to grab from time (i.e. _length_). 

```{r}
input <- 'http://thredds.daac.ornl.gov/thredds/dodsC/ornldaac/1220/mstmip_driver_global_hd_climate_tair_2004_v1.nc4'
nc <- nc_open(input)
lat <- ncvar_get(nc, "lat")
lon <- ncvar_get(nc, "lon")
time <- ncvar_get(nc, "time")

# These lat/lon coordinates are at a .5 degree resolution
# Closest coords to Boston are 42.25 and 71.25
bos_lat = which(lat == 42.25)
bos_lon = which(lon == 71.25)

# print names of variables
attributes(nc$var)$names

# get air temperature data for our lat/lon subset
tair <- ncvar_get(nc,"tair", c(bos_lon,bos_lat,1), c(1,1,length(time)))

# plot all 1464 data points
plot(time, tair)

nc_close(nc)
```



## Question 5

Plot EVI versus time and compare to the CO2 flux observations.

```{r}
MODISTools::mt_products()
MODISTools::mt_bands(product="MOD13Q1")

WC_file = "MODIS.WillowCreek.RData"
if(file.exists(WC_file)){
  load(WC_file)
} else {
  subset <- MODISTools::mt_subset(product = "MOD13Q1",
                                band = "250m_16_days_EVI",
                                lat=46.0827,
                                lon=-89.9792,
                                start="2012-01-01",
                                end="2012-12-31",
                                km_lr = 1,
                                km_ab = 1,
                                site_name = "WillowCreek")
  save(subset,file=WC_file)
}
subset$header
head(subset$data)

## average EVI spatially & use 'scale' to set units
EVI = tapply(subset$data$data, subset$data$calendar_date, mean,na.rm=TRUE) * as.numeric(subset$header$scale)
time = as.Date(names(EVI))

plot(time, EVI, type='l')
```
```{r}
# compare to NEE data
wlef = nc_open("US-PFa-WLEF-TallTowerClean-2012-L0-vFeb2013.nc")
NEE = ncvar_get(wlef,"NEE_co2")    ## NEE data

## matrix dimensions
height = ncvar_get(wlef,"M_lvl")  
doy = ncvar_get(wlef,"time")  # day of year

## close file connection
nc_close(wlef)

# plot NEE
for(i in 1:3){
plot(doy,filter(NEE[i,],rep(1/24,24)),type='l',main=paste("Height =",height[i],"m"))
}
```
NEE (carbon flux) data is only present for the middle of the year, but appears to have an inverse relationship with EVI. This makes sense, because negative NEE values represent an increased ratio of photosynthesis to respiration, and positive EVI values represent more vegetation. There is also a lot more temporal variation within the NEE dataset, which makes sense since photosynthesis and respiration rates fluctuate on a daily timescale.

##Question #6

Imagine you are working with the full FIA database and want to ensure that the data you are using is always up to date. However, the total size of the database is large, the USFS server is slow, and you don't want to completely delete and reinstall the database every day when only a small percentage of the data changes in any update. 

* Write out the pseudocode/outline for how to keep the files up to date
* Write out what the cron table would look like to schedule this job (assume the update only needs to be done weekly)

#### cron tab running at midnight every Sunday.
```
MAILTO=zrwerbin@bu.edu
0 0 * * 7 /home/zrwerbin/update_fia_weekly.r
```
#### PSEUDOCODE - GET DATABASE UPDATES FROM FIA
```{r}
# update_fia_weekly.r

## Step 1. Initial download of FIA database
# grab raw html
fia_html <- getURL("https://apps.fs.usda.gov/fia/datamart/CSV/datamart_csv.html")  
# get the 3rd table on this webpage - list of file info
fia_table = readHTMLTable(fia_html)[[3]]   
# download all data
# [loop through each link from fia_table and download]

## Step 2. Determine which files have been updated
update = as.Date(fia_table[,"Last Modified Date"])
# print current date
today <- Sys.Date()
# get date of 7 days ago
lastweek <- today - 7
# get df of every file modified in past week
recent.df <- fia_table[which(update > lastweek),]

## Step 3. Download only files from past week
# download new data
# [loop through each link from the recent.df and download]
```

